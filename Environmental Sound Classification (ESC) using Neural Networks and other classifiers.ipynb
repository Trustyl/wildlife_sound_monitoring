{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##safety executable check (for conda env)\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Environment Sound Classification Dataset (https://github.com/karoldvl/ESC-50)\n",
    "Testing feature extraction and several classifiers with the ECS-10 data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Audio Classes: ', ['001 - Dog bark', '002 - Rain', '003 - Sea waves', '004 - Baby cry', '005 - Clock tick', '006 - Person sneeze', '007 - Helicopter', '008 - Chainsaw', '009 - Rooster', '010 - Fire crackling'])\n"
     ]
    }
   ],
   "source": [
    "# coding= UTF-8\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import soundfile as sf\n",
    "\n",
    "##Return audio features \n",
    "def feature_extraction(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    if X.ndim > 1:\n",
    "        X = X[:,0]\n",
    "    X = X.T\n",
    "    \n",
    "    # Get features   \n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0) #40 values\n",
    "    #zcr = np.mean(librosa.feature.zero_crossing_rate)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0) #tonal centroid features\n",
    "    \n",
    "    ##Return computed features\n",
    "    return mfccs, chroma, mel, contrast, tonnetz\n",
    "    \n",
    "# Process audio files: Return arrays with features and labels\n",
    "def parse_audio_files(parent_dir, sub_dirs, file_ext='*.ogg'): ## .ogg audio format\n",
    "    features, labels = np.empty((0,193)), np.empty(0) # 193 features total. This can vary\n",
    "    \n",
    "    for label, sub_dir in enumerate(sub_dirs): ##The enumerate() function adds a counter to an iterable.\n",
    "        for file_name in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)): ##parent is data, sub_dirs are the classes\n",
    "            try:\n",
    "                mfccs, chroma, mel, contrast, tonnetz = feature_extraction(file_name)\n",
    "            except Exception as e:\n",
    "                print(\"[Error] there was an error in feature extraction. %s\" % (e))\n",
    "                continue\n",
    "                \n",
    "            extracted_features = np.hstack([mfccs,chroma, mel, contrast, tonnetz]) #Stack arrays in sequence horizontally (column wise)\n",
    "            features = np.vstack([features, extracted_features]) #Stack arrays in sequence vertically (row wise).\n",
    "            labels = np.append(labels, label)\n",
    "        print(\"Extracted features from %s, done\" % (sub_dir))\n",
    "    return np.array(features), np.array(labels, dtype = np.int) ## arrays with features and corresponding labels for each audio\n",
    "\n",
    "def one_hot_encode(labels): ##Check this hot encode\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode\n",
    "\n",
    "# Read sub-directories (audio classes)\n",
    "audio_directories = os.listdir(\"audio-data/\")\n",
    "audio_directories.sort()\n",
    "print('Audio Classes: ', audio_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features from 001 - Dog bark, done\n",
      "Extracted features from 002 - Rain, done\n",
      "Extracted features from 003 - Sea waves, done\n",
      "Extracted features from 004 - Baby cry, done\n",
      "Extracted features from 005 - Clock tick, done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hackerman/anaconda2/envs/anaconda2_py27/lib/python2.7/site-packages/librosa/core/pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features from 006 - Person sneeze, done\n",
      "Extracted features from 007 - Helicopter, done\n",
      "Extracted features from 008 - Chainsaw, done\n",
      "Extracted features from 009 - Rooster, done\n",
      "Extracted features from 010 - Fire crackling, done\n"
     ]
    }
   ],
   "source": [
    "##Get labels and features\n",
    "features, labels = parse_audio_files('audio-data', audio_directories) #(parent dir,sub dirs)\n",
    "np.save('feat.npy', features) ##NumPy array file created. Files are binary files to store numpy arrays\n",
    "np.save('label.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dog bark' 'Rain' 'Sea waves' 'Baby cry' 'Clock tick' 'Person sneeze'\n",
      " 'Helicopter' 'Chainsaw' 'Rooster' 'Fire crackling']\n"
     ]
    }
   ],
   "source": [
    "# Label integer encoding \n",
    "labels = np.load('label.npy') # 10 labels total\n",
    "#print(labels)\n",
    "\n",
    "# For future label de-encoding\n",
    "label_classes = np.array(['Dog bark','Rain','Sea waves','Baby cry','Clock tick','Person sneeze','Helicopter','Chainsaw','Rooster',\n",
    "                          'Fire crackling'])\n",
    "print(label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "[[-3.12936138e+02  1.53756654e+02 -8.42777452e+01 ...  5.63282588e-02\n",
      "   2.08516431e-02 -1.87674272e-02]\n",
      " [-3.20112853e+02  1.42200505e+02 -4.77227058e+01 ...  7.24356147e-02\n",
      "   1.58665382e-02 -2.03768118e-03]\n",
      " [-5.69369487e+02  3.15002805e+01  9.34787663e-01 ... -9.74065657e-03\n",
      "  -2.01712929e-03  2.35111669e-02]\n",
      " ...\n",
      " [-2.86730051e+02  6.26902425e+01  1.85678427e+01 ...  9.72864029e-03\n",
      "   8.37429690e-03  1.61102826e-02]\n",
      " [-3.26840023e+02  9.06312882e+01  7.19092342e+01 ...  5.48059287e-02\n",
      "   1.16528212e-02  9.53665151e-03]\n",
      " [-2.96461065e+02  7.54222837e+01  1.30356668e+01 ...  9.11465921e-02\n",
      "   2.24434430e-02  1.89593115e-02]]\n"
     ]
    }
   ],
   "source": [
    "features= np.load('feat.npy')\n",
    "print(len(features)) # 400 features total\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Audio class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-359.907670</td>\n",
       "      <td>176.984312</td>\n",
       "      <td>-28.405258</td>\n",
       "      <td>-77.346419</td>\n",
       "      <td>-23.394936</td>\n",
       "      <td>-8.336801</td>\n",
       "      <td>-27.351105</td>\n",
       "      <td>-21.991833</td>\n",
       "      <td>-0.599230</td>\n",
       "      <td>3.381202</td>\n",
       "      <td>-1.022481</td>\n",
       "      <td>10.735957</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-364.606377</td>\n",
       "      <td>160.027782</td>\n",
       "      <td>-10.176146</td>\n",
       "      <td>-49.205103</td>\n",
       "      <td>-47.292736</td>\n",
       "      <td>-24.890667</td>\n",
       "      <td>-23.486948</td>\n",
       "      <td>-17.898796</td>\n",
       "      <td>-8.003186</td>\n",
       "      <td>-14.999630</td>\n",
       "      <td>-10.643615</td>\n",
       "      <td>3.901059</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-569.158646</td>\n",
       "      <td>30.929713</td>\n",
       "      <td>5.359668</td>\n",
       "      <td>0.996636</td>\n",
       "      <td>1.891372</td>\n",
       "      <td>2.343744</td>\n",
       "      <td>-1.432037</td>\n",
       "      <td>0.311365</td>\n",
       "      <td>2.254691</td>\n",
       "      <td>1.684495</td>\n",
       "      <td>1.046472</td>\n",
       "      <td>3.674637</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-419.991445</td>\n",
       "      <td>158.331816</td>\n",
       "      <td>-5.460790</td>\n",
       "      <td>-2.632195</td>\n",
       "      <td>10.701890</td>\n",
       "      <td>13.474268</td>\n",
       "      <td>9.900013</td>\n",
       "      <td>7.775121</td>\n",
       "      <td>7.893351</td>\n",
       "      <td>6.628814</td>\n",
       "      <td>7.100705</td>\n",
       "      <td>9.033844</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-331.014025</td>\n",
       "      <td>124.460048</td>\n",
       "      <td>-20.819073</td>\n",
       "      <td>-29.484597</td>\n",
       "      <td>-36.932396</td>\n",
       "      <td>-7.367482</td>\n",
       "      <td>-16.957456</td>\n",
       "      <td>-7.053254</td>\n",
       "      <td>-2.576714</td>\n",
       "      <td>-8.870600</td>\n",
       "      <td>-0.729187</td>\n",
       "      <td>7.784047</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-393.339552</td>\n",
       "      <td>78.339811</td>\n",
       "      <td>4.843938</td>\n",
       "      <td>-7.676779</td>\n",
       "      <td>-12.608208</td>\n",
       "      <td>-4.106547</td>\n",
       "      <td>-6.196130</td>\n",
       "      <td>-4.184623</td>\n",
       "      <td>-0.346573</td>\n",
       "      <td>2.493216</td>\n",
       "      <td>-0.460569</td>\n",
       "      <td>-3.794559</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-387.847069</td>\n",
       "      <td>112.019940</td>\n",
       "      <td>11.978814</td>\n",
       "      <td>-11.534052</td>\n",
       "      <td>-15.415533</td>\n",
       "      <td>0.335486</td>\n",
       "      <td>-1.905306</td>\n",
       "      <td>-9.614974</td>\n",
       "      <td>-2.887099</td>\n",
       "      <td>-1.144487</td>\n",
       "      <td>-1.357641</td>\n",
       "      <td>7.187376</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-430.738218</td>\n",
       "      <td>94.625625</td>\n",
       "      <td>11.179736</td>\n",
       "      <td>-7.445123</td>\n",
       "      <td>-2.928210</td>\n",
       "      <td>-3.732962</td>\n",
       "      <td>-8.826805</td>\n",
       "      <td>0.202373</td>\n",
       "      <td>-0.317225</td>\n",
       "      <td>-6.483365</td>\n",
       "      <td>-3.919070</td>\n",
       "      <td>-3.755173</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-466.842492</td>\n",
       "      <td>57.002892</td>\n",
       "      <td>-22.152894</td>\n",
       "      <td>-25.998953</td>\n",
       "      <td>-18.681116</td>\n",
       "      <td>-5.563477</td>\n",
       "      <td>-12.930120</td>\n",
       "      <td>-5.421485</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>-1.839884</td>\n",
       "      <td>-1.536207</td>\n",
       "      <td>-2.283316</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-366.304506</td>\n",
       "      <td>120.885096</td>\n",
       "      <td>12.036169</td>\n",
       "      <td>-3.433854</td>\n",
       "      <td>-24.169440</td>\n",
       "      <td>-13.973501</td>\n",
       "      <td>-6.264325</td>\n",
       "      <td>-3.052905</td>\n",
       "      <td>-12.012035</td>\n",
       "      <td>-6.378392</td>\n",
       "      <td>-2.972299</td>\n",
       "      <td>-8.906094</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-540.882321</td>\n",
       "      <td>49.557678</td>\n",
       "      <td>-2.491799</td>\n",
       "      <td>-7.023349</td>\n",
       "      <td>-6.225777</td>\n",
       "      <td>6.247800</td>\n",
       "      <td>1.571002</td>\n",
       "      <td>1.421077</td>\n",
       "      <td>2.779929</td>\n",
       "      <td>4.209445</td>\n",
       "      <td>1.685205</td>\n",
       "      <td>2.356263</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-393.523419</td>\n",
       "      <td>93.156385</td>\n",
       "      <td>23.004238</td>\n",
       "      <td>3.160594</td>\n",
       "      <td>-9.224882</td>\n",
       "      <td>-12.366414</td>\n",
       "      <td>-16.257689</td>\n",
       "      <td>-3.049603</td>\n",
       "      <td>-6.019779</td>\n",
       "      <td>-12.749218</td>\n",
       "      <td>-2.939721</td>\n",
       "      <td>2.046056</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-251.518405</td>\n",
       "      <td>162.032558</td>\n",
       "      <td>12.211548</td>\n",
       "      <td>0.726063</td>\n",
       "      <td>-16.656684</td>\n",
       "      <td>3.548022</td>\n",
       "      <td>-1.030129</td>\n",
       "      <td>-4.631513</td>\n",
       "      <td>-1.613962</td>\n",
       "      <td>0.837699</td>\n",
       "      <td>1.494230</td>\n",
       "      <td>0.317367</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-326.838738</td>\n",
       "      <td>144.327239</td>\n",
       "      <td>6.447565</td>\n",
       "      <td>-27.834843</td>\n",
       "      <td>-16.875479</td>\n",
       "      <td>7.490809</td>\n",
       "      <td>11.092495</td>\n",
       "      <td>10.361119</td>\n",
       "      <td>19.562269</td>\n",
       "      <td>15.887472</td>\n",
       "      <td>5.118923</td>\n",
       "      <td>-1.282682</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-173.991198</td>\n",
       "      <td>134.575450</td>\n",
       "      <td>-42.192963</td>\n",
       "      <td>-6.033575</td>\n",
       "      <td>-26.256233</td>\n",
       "      <td>-1.971600</td>\n",
       "      <td>-14.490851</td>\n",
       "      <td>11.244717</td>\n",
       "      <td>4.631190</td>\n",
       "      <td>12.047271</td>\n",
       "      <td>0.141807</td>\n",
       "      <td>6.867545</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-410.594556</td>\n",
       "      <td>79.353227</td>\n",
       "      <td>2.590714</td>\n",
       "      <td>-13.479014</td>\n",
       "      <td>-25.084634</td>\n",
       "      <td>-15.225662</td>\n",
       "      <td>-6.087427</td>\n",
       "      <td>-13.530202</td>\n",
       "      <td>-10.442510</td>\n",
       "      <td>-2.410958</td>\n",
       "      <td>-3.960631</td>\n",
       "      <td>-5.403204</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-162.174223</td>\n",
       "      <td>154.726445</td>\n",
       "      <td>-24.796252</td>\n",
       "      <td>-27.214147</td>\n",
       "      <td>-26.419858</td>\n",
       "      <td>-3.651262</td>\n",
       "      <td>-23.759218</td>\n",
       "      <td>-9.270502</td>\n",
       "      <td>-12.027346</td>\n",
       "      <td>-3.213644</td>\n",
       "      <td>-3.673530</td>\n",
       "      <td>0.061474</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-399.709543</td>\n",
       "      <td>98.426396</td>\n",
       "      <td>12.833435</td>\n",
       "      <td>-7.143056</td>\n",
       "      <td>-18.696928</td>\n",
       "      <td>-17.757136</td>\n",
       "      <td>-20.342404</td>\n",
       "      <td>-13.710343</td>\n",
       "      <td>-6.387512</td>\n",
       "      <td>-13.585574</td>\n",
       "      <td>-9.671950</td>\n",
       "      <td>0.338361</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-502.314108</td>\n",
       "      <td>23.699184</td>\n",
       "      <td>-30.881929</td>\n",
       "      <td>1.417022</td>\n",
       "      <td>2.171178</td>\n",
       "      <td>2.311240</td>\n",
       "      <td>7.663824</td>\n",
       "      <td>8.500332</td>\n",
       "      <td>1.034806</td>\n",
       "      <td>-0.677923</td>\n",
       "      <td>2.131254</td>\n",
       "      <td>0.321339</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-371.814298</td>\n",
       "      <td>148.739859</td>\n",
       "      <td>1.139008</td>\n",
       "      <td>-6.799012</td>\n",
       "      <td>4.352360</td>\n",
       "      <td>5.029185</td>\n",
       "      <td>7.453407</td>\n",
       "      <td>1.969978</td>\n",
       "      <td>6.702171</td>\n",
       "      <td>14.429489</td>\n",
       "      <td>8.957919</td>\n",
       "      <td>4.394669</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-395.861763</td>\n",
       "      <td>120.598143</td>\n",
       "      <td>-10.012163</td>\n",
       "      <td>-41.370596</td>\n",
       "      <td>-44.266902</td>\n",
       "      <td>-19.220830</td>\n",
       "      <td>-13.337894</td>\n",
       "      <td>3.367997</td>\n",
       "      <td>-3.701620</td>\n",
       "      <td>-0.528406</td>\n",
       "      <td>5.593808</td>\n",
       "      <td>0.720317</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-357.294547</td>\n",
       "      <td>74.903339</td>\n",
       "      <td>6.216953</td>\n",
       "      <td>-5.213464</td>\n",
       "      <td>0.948881</td>\n",
       "      <td>12.159967</td>\n",
       "      <td>1.224344</td>\n",
       "      <td>-5.258528</td>\n",
       "      <td>-5.127156</td>\n",
       "      <td>-5.417437</td>\n",
       "      <td>-5.635948</td>\n",
       "      <td>-3.069588</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-325.441592</td>\n",
       "      <td>167.794607</td>\n",
       "      <td>-13.000734</td>\n",
       "      <td>9.387221</td>\n",
       "      <td>-11.614192</td>\n",
       "      <td>3.504295</td>\n",
       "      <td>-3.538954</td>\n",
       "      <td>14.719826</td>\n",
       "      <td>13.166767</td>\n",
       "      <td>16.612555</td>\n",
       "      <td>3.383781</td>\n",
       "      <td>-0.969852</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-439.369161</td>\n",
       "      <td>43.640000</td>\n",
       "      <td>8.440763</td>\n",
       "      <td>-3.924500</td>\n",
       "      <td>-11.537148</td>\n",
       "      <td>-8.508559</td>\n",
       "      <td>-6.057363</td>\n",
       "      <td>-8.190636</td>\n",
       "      <td>-6.293608</td>\n",
       "      <td>-2.258003</td>\n",
       "      <td>-1.917748</td>\n",
       "      <td>-3.399405</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-355.410548</td>\n",
       "      <td>78.624848</td>\n",
       "      <td>-80.062616</td>\n",
       "      <td>-35.142855</td>\n",
       "      <td>-33.235695</td>\n",
       "      <td>-29.348386</td>\n",
       "      <td>-17.637758</td>\n",
       "      <td>2.731542</td>\n",
       "      <td>14.141693</td>\n",
       "      <td>2.084361</td>\n",
       "      <td>4.117356</td>\n",
       "      <td>0.623595</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-234.372586</td>\n",
       "      <td>99.578839</td>\n",
       "      <td>-82.462926</td>\n",
       "      <td>-36.530957</td>\n",
       "      <td>-20.341889</td>\n",
       "      <td>7.458714</td>\n",
       "      <td>-2.046457</td>\n",
       "      <td>2.618373</td>\n",
       "      <td>-2.436608</td>\n",
       "      <td>-3.748010</td>\n",
       "      <td>-5.191922</td>\n",
       "      <td>-4.904327</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-444.499809</td>\n",
       "      <td>108.079066</td>\n",
       "      <td>-9.716749</td>\n",
       "      <td>-29.375695</td>\n",
       "      <td>-27.889706</td>\n",
       "      <td>2.131835</td>\n",
       "      <td>-1.083630</td>\n",
       "      <td>4.080067</td>\n",
       "      <td>9.063857</td>\n",
       "      <td>2.918119</td>\n",
       "      <td>8.504166</td>\n",
       "      <td>13.553046</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-581.740034</td>\n",
       "      <td>8.206624</td>\n",
       "      <td>-6.658867</td>\n",
       "      <td>-4.291413</td>\n",
       "      <td>-3.034642</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>-1.964398</td>\n",
       "      <td>-0.271645</td>\n",
       "      <td>0.518169</td>\n",
       "      <td>0.341996</td>\n",
       "      <td>0.905891</td>\n",
       "      <td>-0.009259</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-405.505175</td>\n",
       "      <td>59.754379</td>\n",
       "      <td>-4.077395</td>\n",
       "      <td>-8.551207</td>\n",
       "      <td>-16.864026</td>\n",
       "      <td>-2.251054</td>\n",
       "      <td>-9.876494</td>\n",
       "      <td>-21.600946</td>\n",
       "      <td>-16.974261</td>\n",
       "      <td>-4.904483</td>\n",
       "      <td>-4.200212</td>\n",
       "      <td>-6.215359</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-420.419071</td>\n",
       "      <td>106.566067</td>\n",
       "      <td>15.553363</td>\n",
       "      <td>-14.629801</td>\n",
       "      <td>-20.385269</td>\n",
       "      <td>-16.875465</td>\n",
       "      <td>-14.139864</td>\n",
       "      <td>-13.623018</td>\n",
       "      <td>-15.524470</td>\n",
       "      <td>-9.991046</td>\n",
       "      <td>-5.881340</td>\n",
       "      <td>-0.375083</td>\n",
       "      <td>Dog bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>-300.094697</td>\n",
       "      <td>91.834682</td>\n",
       "      <td>-30.206425</td>\n",
       "      <td>38.955185</td>\n",
       "      <td>-1.753551</td>\n",
       "      <td>23.693829</td>\n",
       "      <td>-5.739989</td>\n",
       "      <td>5.844123</td>\n",
       "      <td>5.521354</td>\n",
       "      <td>9.194872</td>\n",
       "      <td>1.124666</td>\n",
       "      <td>9.511574</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-118.555454</td>\n",
       "      <td>141.601235</td>\n",
       "      <td>-13.002415</td>\n",
       "      <td>68.341045</td>\n",
       "      <td>6.009578</td>\n",
       "      <td>27.019369</td>\n",
       "      <td>19.157938</td>\n",
       "      <td>27.047396</td>\n",
       "      <td>10.832852</td>\n",
       "      <td>8.541903</td>\n",
       "      <td>4.284676</td>\n",
       "      <td>8.322439</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>-466.042948</td>\n",
       "      <td>95.505859</td>\n",
       "      <td>-7.380096</td>\n",
       "      <td>81.324246</td>\n",
       "      <td>-13.459093</td>\n",
       "      <td>30.590572</td>\n",
       "      <td>-9.329276</td>\n",
       "      <td>25.747173</td>\n",
       "      <td>-3.654098</td>\n",
       "      <td>23.029143</td>\n",
       "      <td>-0.212804</td>\n",
       "      <td>16.892872</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>-412.648476</td>\n",
       "      <td>130.760660</td>\n",
       "      <td>16.493260</td>\n",
       "      <td>69.380370</td>\n",
       "      <td>54.264565</td>\n",
       "      <td>42.981106</td>\n",
       "      <td>22.223390</td>\n",
       "      <td>7.566711</td>\n",
       "      <td>7.352468</td>\n",
       "      <td>1.401075</td>\n",
       "      <td>3.057257</td>\n",
       "      <td>0.579075</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>-450.215165</td>\n",
       "      <td>78.638273</td>\n",
       "      <td>31.140161</td>\n",
       "      <td>64.243905</td>\n",
       "      <td>6.187755</td>\n",
       "      <td>36.104622</td>\n",
       "      <td>-3.647072</td>\n",
       "      <td>28.144727</td>\n",
       "      <td>-4.523554</td>\n",
       "      <td>13.197911</td>\n",
       "      <td>2.748313</td>\n",
       "      <td>6.503400</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>-262.838798</td>\n",
       "      <td>110.003953</td>\n",
       "      <td>21.026210</td>\n",
       "      <td>52.077069</td>\n",
       "      <td>-8.068431</td>\n",
       "      <td>38.152427</td>\n",
       "      <td>-17.229969</td>\n",
       "      <td>21.682273</td>\n",
       "      <td>-4.930638</td>\n",
       "      <td>2.267653</td>\n",
       "      <td>3.008735</td>\n",
       "      <td>5.748116</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>-422.213146</td>\n",
       "      <td>128.665364</td>\n",
       "      <td>2.531775</td>\n",
       "      <td>55.202192</td>\n",
       "      <td>-19.051791</td>\n",
       "      <td>35.370599</td>\n",
       "      <td>-20.154725</td>\n",
       "      <td>23.058660</td>\n",
       "      <td>-8.686421</td>\n",
       "      <td>8.705688</td>\n",
       "      <td>-0.756027</td>\n",
       "      <td>8.191142</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>-507.174153</td>\n",
       "      <td>22.817576</td>\n",
       "      <td>31.015973</td>\n",
       "      <td>15.710849</td>\n",
       "      <td>4.526292</td>\n",
       "      <td>14.061358</td>\n",
       "      <td>2.806456</td>\n",
       "      <td>5.065461</td>\n",
       "      <td>2.618662</td>\n",
       "      <td>0.670517</td>\n",
       "      <td>2.822020</td>\n",
       "      <td>4.605235</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>-329.953256</td>\n",
       "      <td>100.782683</td>\n",
       "      <td>2.270215</td>\n",
       "      <td>56.558578</td>\n",
       "      <td>-13.734430</td>\n",
       "      <td>37.107715</td>\n",
       "      <td>-6.904875</td>\n",
       "      <td>23.248728</td>\n",
       "      <td>-4.548081</td>\n",
       "      <td>18.523292</td>\n",
       "      <td>0.818933</td>\n",
       "      <td>9.561117</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>-288.395525</td>\n",
       "      <td>73.795755</td>\n",
       "      <td>-21.384930</td>\n",
       "      <td>60.607331</td>\n",
       "      <td>-11.472752</td>\n",
       "      <td>32.120504</td>\n",
       "      <td>-1.674578</td>\n",
       "      <td>25.734999</td>\n",
       "      <td>1.318557</td>\n",
       "      <td>22.412255</td>\n",
       "      <td>-0.190909</td>\n",
       "      <td>13.708199</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>-195.614058</td>\n",
       "      <td>40.665138</td>\n",
       "      <td>15.420007</td>\n",
       "      <td>65.017738</td>\n",
       "      <td>22.672310</td>\n",
       "      <td>36.763715</td>\n",
       "      <td>2.159317</td>\n",
       "      <td>19.614183</td>\n",
       "      <td>1.554964</td>\n",
       "      <td>14.296776</td>\n",
       "      <td>2.644431</td>\n",
       "      <td>11.267695</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>-355.792827</td>\n",
       "      <td>23.937104</td>\n",
       "      <td>14.717801</td>\n",
       "      <td>23.448826</td>\n",
       "      <td>6.587720</td>\n",
       "      <td>8.049350</td>\n",
       "      <td>4.575113</td>\n",
       "      <td>5.165680</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>1.230308</td>\n",
       "      <td>-2.278786</td>\n",
       "      <td>1.182637</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>-405.908533</td>\n",
       "      <td>110.327779</td>\n",
       "      <td>15.449699</td>\n",
       "      <td>44.679646</td>\n",
       "      <td>25.794437</td>\n",
       "      <td>17.791511</td>\n",
       "      <td>-1.862212</td>\n",
       "      <td>8.933772</td>\n",
       "      <td>8.974281</td>\n",
       "      <td>7.581107</td>\n",
       "      <td>5.355933</td>\n",
       "      <td>7.897992</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>-523.785046</td>\n",
       "      <td>30.472092</td>\n",
       "      <td>-3.299369</td>\n",
       "      <td>76.284799</td>\n",
       "      <td>10.565552</td>\n",
       "      <td>39.687886</td>\n",
       "      <td>14.404874</td>\n",
       "      <td>6.670390</td>\n",
       "      <td>2.734136</td>\n",
       "      <td>-1.241627</td>\n",
       "      <td>5.654907</td>\n",
       "      <td>6.377897</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>-274.051274</td>\n",
       "      <td>76.631909</td>\n",
       "      <td>-18.934463</td>\n",
       "      <td>43.620297</td>\n",
       "      <td>-8.988119</td>\n",
       "      <td>40.505181</td>\n",
       "      <td>-7.529687</td>\n",
       "      <td>24.247380</td>\n",
       "      <td>-13.162683</td>\n",
       "      <td>7.987560</td>\n",
       "      <td>-12.190230</td>\n",
       "      <td>15.334091</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>-320.269833</td>\n",
       "      <td>121.692833</td>\n",
       "      <td>18.068000</td>\n",
       "      <td>52.281800</td>\n",
       "      <td>-12.309194</td>\n",
       "      <td>40.607972</td>\n",
       "      <td>-19.895872</td>\n",
       "      <td>23.491210</td>\n",
       "      <td>-6.092693</td>\n",
       "      <td>4.972782</td>\n",
       "      <td>1.515241</td>\n",
       "      <td>6.577789</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>-305.585611</td>\n",
       "      <td>111.803554</td>\n",
       "      <td>-10.204981</td>\n",
       "      <td>53.519572</td>\n",
       "      <td>11.787420</td>\n",
       "      <td>38.870977</td>\n",
       "      <td>-0.453119</td>\n",
       "      <td>25.138845</td>\n",
       "      <td>6.437259</td>\n",
       "      <td>19.067780</td>\n",
       "      <td>8.359091</td>\n",
       "      <td>14.577221</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>-423.173648</td>\n",
       "      <td>70.572671</td>\n",
       "      <td>13.853737</td>\n",
       "      <td>59.451617</td>\n",
       "      <td>6.116873</td>\n",
       "      <td>43.431000</td>\n",
       "      <td>3.880048</td>\n",
       "      <td>29.620471</td>\n",
       "      <td>6.665323</td>\n",
       "      <td>21.163956</td>\n",
       "      <td>8.565681</td>\n",
       "      <td>14.696945</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-306.183278</td>\n",
       "      <td>106.442452</td>\n",
       "      <td>-6.884659</td>\n",
       "      <td>56.481412</td>\n",
       "      <td>-13.500079</td>\n",
       "      <td>38.301896</td>\n",
       "      <td>-9.587440</td>\n",
       "      <td>22.162919</td>\n",
       "      <td>-6.621620</td>\n",
       "      <td>17.411821</td>\n",
       "      <td>-1.154595</td>\n",
       "      <td>8.302781</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>-377.900565</td>\n",
       "      <td>92.241812</td>\n",
       "      <td>7.801299</td>\n",
       "      <td>38.116885</td>\n",
       "      <td>14.386035</td>\n",
       "      <td>32.430464</td>\n",
       "      <td>1.616616</td>\n",
       "      <td>24.374035</td>\n",
       "      <td>3.367760</td>\n",
       "      <td>22.599554</td>\n",
       "      <td>9.347721</td>\n",
       "      <td>11.433527</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>-506.713039</td>\n",
       "      <td>101.703976</td>\n",
       "      <td>-19.196165</td>\n",
       "      <td>13.428451</td>\n",
       "      <td>-23.891664</td>\n",
       "      <td>3.358840</td>\n",
       "      <td>-30.604990</td>\n",
       "      <td>-3.381933</td>\n",
       "      <td>-23.926288</td>\n",
       "      <td>-2.331027</td>\n",
       "      <td>-10.448816</td>\n",
       "      <td>9.450192</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>-470.395324</td>\n",
       "      <td>42.142829</td>\n",
       "      <td>10.368376</td>\n",
       "      <td>23.412928</td>\n",
       "      <td>4.855624</td>\n",
       "      <td>16.073767</td>\n",
       "      <td>2.211797</td>\n",
       "      <td>12.296866</td>\n",
       "      <td>2.209212</td>\n",
       "      <td>6.895662</td>\n",
       "      <td>2.167898</td>\n",
       "      <td>3.449607</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-210.660504</td>\n",
       "      <td>63.282024</td>\n",
       "      <td>76.253690</td>\n",
       "      <td>44.304715</td>\n",
       "      <td>0.083116</td>\n",
       "      <td>17.451969</td>\n",
       "      <td>16.969995</td>\n",
       "      <td>17.234242</td>\n",
       "      <td>11.220695</td>\n",
       "      <td>13.011647</td>\n",
       "      <td>7.009336</td>\n",
       "      <td>9.446524</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>-447.669247</td>\n",
       "      <td>31.767774</td>\n",
       "      <td>-5.078535</td>\n",
       "      <td>88.273678</td>\n",
       "      <td>1.325207</td>\n",
       "      <td>52.819409</td>\n",
       "      <td>2.014080</td>\n",
       "      <td>17.642716</td>\n",
       "      <td>-6.726231</td>\n",
       "      <td>2.875245</td>\n",
       "      <td>-5.123648</td>\n",
       "      <td>7.945252</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>-307.256711</td>\n",
       "      <td>131.444103</td>\n",
       "      <td>11.867768</td>\n",
       "      <td>83.098990</td>\n",
       "      <td>49.317779</td>\n",
       "      <td>46.967715</td>\n",
       "      <td>19.804348</td>\n",
       "      <td>8.216665</td>\n",
       "      <td>7.523388</td>\n",
       "      <td>0.642674</td>\n",
       "      <td>5.716594</td>\n",
       "      <td>-0.382634</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-384.548580</td>\n",
       "      <td>101.746224</td>\n",
       "      <td>32.108672</td>\n",
       "      <td>56.579118</td>\n",
       "      <td>9.314567</td>\n",
       "      <td>28.069054</td>\n",
       "      <td>7.746387</td>\n",
       "      <td>23.345065</td>\n",
       "      <td>4.440521</td>\n",
       "      <td>17.530255</td>\n",
       "      <td>2.411567</td>\n",
       "      <td>12.412046</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-313.443460</td>\n",
       "      <td>90.446683</td>\n",
       "      <td>-21.633311</td>\n",
       "      <td>47.817287</td>\n",
       "      <td>-8.669852</td>\n",
       "      <td>41.148008</td>\n",
       "      <td>-10.518073</td>\n",
       "      <td>24.209262</td>\n",
       "      <td>-17.152323</td>\n",
       "      <td>16.739314</td>\n",
       "      <td>-4.695033</td>\n",
       "      <td>12.009720</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-331.524556</td>\n",
       "      <td>85.388189</td>\n",
       "      <td>4.587339</td>\n",
       "      <td>37.451762</td>\n",
       "      <td>8.042072</td>\n",
       "      <td>36.921207</td>\n",
       "      <td>-4.587271</td>\n",
       "      <td>6.069305</td>\n",
       "      <td>11.110454</td>\n",
       "      <td>12.464393</td>\n",
       "      <td>-8.986749</td>\n",
       "      <td>6.567881</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-356.582244</td>\n",
       "      <td>90.949527</td>\n",
       "      <td>55.646832</td>\n",
       "      <td>53.836137</td>\n",
       "      <td>-1.281263</td>\n",
       "      <td>45.125923</td>\n",
       "      <td>-0.900402</td>\n",
       "      <td>34.982599</td>\n",
       "      <td>2.979594</td>\n",
       "      <td>17.404423</td>\n",
       "      <td>0.841587</td>\n",
       "      <td>7.959205</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>-331.953644</td>\n",
       "      <td>94.033859</td>\n",
       "      <td>-0.729796</td>\n",
       "      <td>38.716156</td>\n",
       "      <td>6.037472</td>\n",
       "      <td>34.680409</td>\n",
       "      <td>-3.117212</td>\n",
       "      <td>29.623629</td>\n",
       "      <td>-4.359803</td>\n",
       "      <td>28.731081</td>\n",
       "      <td>4.322773</td>\n",
       "      <td>14.116313</td>\n",
       "      <td>Fire crackling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1          2          3          4          5  \\\n",
       "0   -359.907670  176.984312 -28.405258 -77.346419 -23.394936  -8.336801   \n",
       "1   -364.606377  160.027782 -10.176146 -49.205103 -47.292736 -24.890667   \n",
       "2   -569.158646   30.929713   5.359668   0.996636   1.891372   2.343744   \n",
       "3   -419.991445  158.331816  -5.460790  -2.632195  10.701890  13.474268   \n",
       "4   -331.014025  124.460048 -20.819073 -29.484597 -36.932396  -7.367482   \n",
       "5   -393.339552   78.339811   4.843938  -7.676779 -12.608208  -4.106547   \n",
       "6   -387.847069  112.019940  11.978814 -11.534052 -15.415533   0.335486   \n",
       "7   -430.738218   94.625625  11.179736  -7.445123  -2.928210  -3.732962   \n",
       "8   -466.842492   57.002892 -22.152894 -25.998953 -18.681116  -5.563477   \n",
       "9   -366.304506  120.885096  12.036169  -3.433854 -24.169440 -13.973501   \n",
       "10  -540.882321   49.557678  -2.491799  -7.023349  -6.225777   6.247800   \n",
       "11  -393.523419   93.156385  23.004238   3.160594  -9.224882 -12.366414   \n",
       "12  -251.518405  162.032558  12.211548   0.726063 -16.656684   3.548022   \n",
       "13  -326.838738  144.327239   6.447565 -27.834843 -16.875479   7.490809   \n",
       "14  -173.991198  134.575450 -42.192963  -6.033575 -26.256233  -1.971600   \n",
       "15  -410.594556   79.353227   2.590714 -13.479014 -25.084634 -15.225662   \n",
       "16  -162.174223  154.726445 -24.796252 -27.214147 -26.419858  -3.651262   \n",
       "17  -399.709543   98.426396  12.833435  -7.143056 -18.696928 -17.757136   \n",
       "18  -502.314108   23.699184 -30.881929   1.417022   2.171178   2.311240   \n",
       "19  -371.814298  148.739859   1.139008  -6.799012   4.352360   5.029185   \n",
       "20  -395.861763  120.598143 -10.012163 -41.370596 -44.266902 -19.220830   \n",
       "21  -357.294547   74.903339   6.216953  -5.213464   0.948881  12.159967   \n",
       "22  -325.441592  167.794607 -13.000734   9.387221 -11.614192   3.504295   \n",
       "23  -439.369161   43.640000   8.440763  -3.924500 -11.537148  -8.508559   \n",
       "24  -355.410548   78.624848 -80.062616 -35.142855 -33.235695 -29.348386   \n",
       "25  -234.372586   99.578839 -82.462926 -36.530957 -20.341889   7.458714   \n",
       "26  -444.499809  108.079066  -9.716749 -29.375695 -27.889706   2.131835   \n",
       "27  -581.740034    8.206624  -6.658867  -4.291413  -3.034642   0.259200   \n",
       "28  -405.505175   59.754379  -4.077395  -8.551207 -16.864026  -2.251054   \n",
       "29  -420.419071  106.566067  15.553363 -14.629801 -20.385269 -16.875465   \n",
       "..          ...         ...        ...        ...        ...        ...   \n",
       "370 -300.094697   91.834682 -30.206425  38.955185  -1.753551  23.693829   \n",
       "371 -118.555454  141.601235 -13.002415  68.341045   6.009578  27.019369   \n",
       "372 -466.042948   95.505859  -7.380096  81.324246 -13.459093  30.590572   \n",
       "373 -412.648476  130.760660  16.493260  69.380370  54.264565  42.981106   \n",
       "374 -450.215165   78.638273  31.140161  64.243905   6.187755  36.104622   \n",
       "375 -262.838798  110.003953  21.026210  52.077069  -8.068431  38.152427   \n",
       "376 -422.213146  128.665364   2.531775  55.202192 -19.051791  35.370599   \n",
       "377 -507.174153   22.817576  31.015973  15.710849   4.526292  14.061358   \n",
       "378 -329.953256  100.782683   2.270215  56.558578 -13.734430  37.107715   \n",
       "379 -288.395525   73.795755 -21.384930  60.607331 -11.472752  32.120504   \n",
       "380 -195.614058   40.665138  15.420007  65.017738  22.672310  36.763715   \n",
       "381 -355.792827   23.937104  14.717801  23.448826   6.587720   8.049350   \n",
       "382 -405.908533  110.327779  15.449699  44.679646  25.794437  17.791511   \n",
       "383 -523.785046   30.472092  -3.299369  76.284799  10.565552  39.687886   \n",
       "384 -274.051274   76.631909 -18.934463  43.620297  -8.988119  40.505181   \n",
       "385 -320.269833  121.692833  18.068000  52.281800 -12.309194  40.607972   \n",
       "386 -305.585611  111.803554 -10.204981  53.519572  11.787420  38.870977   \n",
       "387 -423.173648   70.572671  13.853737  59.451617   6.116873  43.431000   \n",
       "388 -306.183278  106.442452  -6.884659  56.481412 -13.500079  38.301896   \n",
       "389 -377.900565   92.241812   7.801299  38.116885  14.386035  32.430464   \n",
       "390 -506.713039  101.703976 -19.196165  13.428451 -23.891664   3.358840   \n",
       "391 -470.395324   42.142829  10.368376  23.412928   4.855624  16.073767   \n",
       "392 -210.660504   63.282024  76.253690  44.304715   0.083116  17.451969   \n",
       "393 -447.669247   31.767774  -5.078535  88.273678   1.325207  52.819409   \n",
       "394 -307.256711  131.444103  11.867768  83.098990  49.317779  46.967715   \n",
       "395 -384.548580  101.746224  32.108672  56.579118   9.314567  28.069054   \n",
       "396 -313.443460   90.446683 -21.633311  47.817287  -8.669852  41.148008   \n",
       "397 -331.524556   85.388189   4.587339  37.451762   8.042072  36.921207   \n",
       "398 -356.582244   90.949527  55.646832  53.836137  -1.281263  45.125923   \n",
       "399 -331.953644   94.033859  -0.729796  38.716156   6.037472  34.680409   \n",
       "\n",
       "             6          7          8          9         10         11  \\\n",
       "0   -27.351105 -21.991833  -0.599230   3.381202  -1.022481  10.735957   \n",
       "1   -23.486948 -17.898796  -8.003186 -14.999630 -10.643615   3.901059   \n",
       "2    -1.432037   0.311365   2.254691   1.684495   1.046472   3.674637   \n",
       "3     9.900013   7.775121   7.893351   6.628814   7.100705   9.033844   \n",
       "4   -16.957456  -7.053254  -2.576714  -8.870600  -0.729187   7.784047   \n",
       "5    -6.196130  -4.184623  -0.346573   2.493216  -0.460569  -3.794559   \n",
       "6    -1.905306  -9.614974  -2.887099  -1.144487  -1.357641   7.187376   \n",
       "7    -8.826805   0.202373  -0.317225  -6.483365  -3.919070  -3.755173   \n",
       "8   -12.930120  -5.421485   0.002677  -1.839884  -1.536207  -2.283316   \n",
       "9    -6.264325  -3.052905 -12.012035  -6.378392  -2.972299  -8.906094   \n",
       "10    1.571002   1.421077   2.779929   4.209445   1.685205   2.356263   \n",
       "11  -16.257689  -3.049603  -6.019779 -12.749218  -2.939721   2.046056   \n",
       "12   -1.030129  -4.631513  -1.613962   0.837699   1.494230   0.317367   \n",
       "13   11.092495  10.361119  19.562269  15.887472   5.118923  -1.282682   \n",
       "14  -14.490851  11.244717   4.631190  12.047271   0.141807   6.867545   \n",
       "15   -6.087427 -13.530202 -10.442510  -2.410958  -3.960631  -5.403204   \n",
       "16  -23.759218  -9.270502 -12.027346  -3.213644  -3.673530   0.061474   \n",
       "17  -20.342404 -13.710343  -6.387512 -13.585574  -9.671950   0.338361   \n",
       "18    7.663824   8.500332   1.034806  -0.677923   2.131254   0.321339   \n",
       "19    7.453407   1.969978   6.702171  14.429489   8.957919   4.394669   \n",
       "20  -13.337894   3.367997  -3.701620  -0.528406   5.593808   0.720317   \n",
       "21    1.224344  -5.258528  -5.127156  -5.417437  -5.635948  -3.069588   \n",
       "22   -3.538954  14.719826  13.166767  16.612555   3.383781  -0.969852   \n",
       "23   -6.057363  -8.190636  -6.293608  -2.258003  -1.917748  -3.399405   \n",
       "24  -17.637758   2.731542  14.141693   2.084361   4.117356   0.623595   \n",
       "25   -2.046457   2.618373  -2.436608  -3.748010  -5.191922  -4.904327   \n",
       "26   -1.083630   4.080067   9.063857   2.918119   8.504166  13.553046   \n",
       "27   -1.964398  -0.271645   0.518169   0.341996   0.905891  -0.009259   \n",
       "28   -9.876494 -21.600946 -16.974261  -4.904483  -4.200212  -6.215359   \n",
       "29  -14.139864 -13.623018 -15.524470  -9.991046  -5.881340  -0.375083   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "370  -5.739989   5.844123   5.521354   9.194872   1.124666   9.511574   \n",
       "371  19.157938  27.047396  10.832852   8.541903   4.284676   8.322439   \n",
       "372  -9.329276  25.747173  -3.654098  23.029143  -0.212804  16.892872   \n",
       "373  22.223390   7.566711   7.352468   1.401075   3.057257   0.579075   \n",
       "374  -3.647072  28.144727  -4.523554  13.197911   2.748313   6.503400   \n",
       "375 -17.229969  21.682273  -4.930638   2.267653   3.008735   5.748116   \n",
       "376 -20.154725  23.058660  -8.686421   8.705688  -0.756027   8.191142   \n",
       "377   2.806456   5.065461   2.618662   0.670517   2.822020   4.605235   \n",
       "378  -6.904875  23.248728  -4.548081  18.523292   0.818933   9.561117   \n",
       "379  -1.674578  25.734999   1.318557  22.412255  -0.190909  13.708199   \n",
       "380   2.159317  19.614183   1.554964  14.296776   2.644431  11.267695   \n",
       "381   4.575113   5.165680   0.543515   1.230308  -2.278786   1.182637   \n",
       "382  -1.862212   8.933772   8.974281   7.581107   5.355933   7.897992   \n",
       "383  14.404874   6.670390   2.734136  -1.241627   5.654907   6.377897   \n",
       "384  -7.529687  24.247380 -13.162683   7.987560 -12.190230  15.334091   \n",
       "385 -19.895872  23.491210  -6.092693   4.972782   1.515241   6.577789   \n",
       "386  -0.453119  25.138845   6.437259  19.067780   8.359091  14.577221   \n",
       "387   3.880048  29.620471   6.665323  21.163956   8.565681  14.696945   \n",
       "388  -9.587440  22.162919  -6.621620  17.411821  -1.154595   8.302781   \n",
       "389   1.616616  24.374035   3.367760  22.599554   9.347721  11.433527   \n",
       "390 -30.604990  -3.381933 -23.926288  -2.331027 -10.448816   9.450192   \n",
       "391   2.211797  12.296866   2.209212   6.895662   2.167898   3.449607   \n",
       "392  16.969995  17.234242  11.220695  13.011647   7.009336   9.446524   \n",
       "393   2.014080  17.642716  -6.726231   2.875245  -5.123648   7.945252   \n",
       "394  19.804348   8.216665   7.523388   0.642674   5.716594  -0.382634   \n",
       "395   7.746387  23.345065   4.440521  17.530255   2.411567  12.412046   \n",
       "396 -10.518073  24.209262 -17.152323  16.739314  -4.695033  12.009720   \n",
       "397  -4.587271   6.069305  11.110454  12.464393  -8.986749   6.567881   \n",
       "398  -0.900402  34.982599   2.979594  17.404423   0.841587   7.959205   \n",
       "399  -3.117212  29.623629  -4.359803  28.731081   4.322773  14.116313   \n",
       "\n",
       "        Audio class  \n",
       "0          Dog bark  \n",
       "1          Dog bark  \n",
       "2          Dog bark  \n",
       "3          Dog bark  \n",
       "4          Dog bark  \n",
       "5          Dog bark  \n",
       "6          Dog bark  \n",
       "7          Dog bark  \n",
       "8          Dog bark  \n",
       "9          Dog bark  \n",
       "10         Dog bark  \n",
       "11         Dog bark  \n",
       "12         Dog bark  \n",
       "13         Dog bark  \n",
       "14         Dog bark  \n",
       "15         Dog bark  \n",
       "16         Dog bark  \n",
       "17         Dog bark  \n",
       "18         Dog bark  \n",
       "19         Dog bark  \n",
       "20         Dog bark  \n",
       "21         Dog bark  \n",
       "22         Dog bark  \n",
       "23         Dog bark  \n",
       "24         Dog bark  \n",
       "25         Dog bark  \n",
       "26         Dog bark  \n",
       "27         Dog bark  \n",
       "28         Dog bark  \n",
       "29         Dog bark  \n",
       "..              ...  \n",
       "370  Fire crackling  \n",
       "371  Fire crackling  \n",
       "372  Fire crackling  \n",
       "373  Fire crackling  \n",
       "374  Fire crackling  \n",
       "375  Fire crackling  \n",
       "376  Fire crackling  \n",
       "377  Fire crackling  \n",
       "378  Fire crackling  \n",
       "379  Fire crackling  \n",
       "380  Fire crackling  \n",
       "381  Fire crackling  \n",
       "382  Fire crackling  \n",
       "383  Fire crackling  \n",
       "384  Fire crackling  \n",
       "385  Fire crackling  \n",
       "386  Fire crackling  \n",
       "387  Fire crackling  \n",
       "388  Fire crackling  \n",
       "389  Fire crackling  \n",
       "390  Fire crackling  \n",
       "391  Fire crackling  \n",
       "392  Fire crackling  \n",
       "393  Fire crackling  \n",
       "394  Fire crackling  \n",
       "395  Fire crackling  \n",
       "396  Fire crackling  \n",
       "397  Fire crackling  \n",
       "398  Fire crackling  \n",
       "399  Fire crackling  \n",
       "\n",
       "[400 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pandas dataframe with 193 features variables for each audio\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "# Add a new column for class (label), this is our target\n",
    "df['Audio class'] = pd.Categorical.from_codes(labels, label_classes)\n",
    "\n",
    "df[[0,1,2,3,4,5,6,7,8,9,10,11,'Audio class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[3 2 0 4 6 1 3 6 3 5 5 3 2 8 0 1 7 3 5 4 4 6 3 5 4 1 1 0 2 2 4 2 8 5 1 6 2\n",
      " 4 4 9 2 0 0 0 7 1 9 7 3 5 1 3 7 4 7 2 1 5 8 1 1 7 0 8 4 6 8 0 4 7 0 6 1 0\n",
      " 2 0 9 7 9 8 8 1 1 1 7 5 3 9 9 8 9 6 1 8 4 3 3 4 7 9 6 1 7 3 1 8 2 8 3 7 2\n",
      " 1 7 0 9 4 6 1 5 8 2 5 5 5 9 3 0 6 8 4 3 9 1 6 5 2 2 0 8 0 4 4 5 2 6 2 8 6\n",
      " 5 2 8 3 6 2 5 3 4 3 9 7]\n",
      "\n",
      "Actual\n",
      "[3 7 8 4 6 1 3 6 3 5 0 3 2 8 0 1 7 3 5 4 4 6 3 8 4 7 1 0 7 2 4 7 8 5 1 1 2\n",
      " 4 4 9 2 8 0 0 7 1 9 6 3 5 6 3 7 4 7 7 1 0 0 1 1 6 0 8 4 9 3 0 3 7 0 6 1 0\n",
      " 2 0 9 7 9 8 8 1 1 2 7 5 3 9 9 8 9 6 1 0 4 3 3 9 6 9 6 1 7 3 7 8 2 8 3 7 2\n",
      " 2 7 0 9 5 6 1 5 8 2 5 5 5 9 3 0 6 8 4 3 9 1 6 5 2 7 4 8 0 4 4 0 2 6 2 8 6\n",
      " 5 2 8 3 7 2 5 3 6 3 4 7]\n",
      "\n",
      "[ 0  5  8  0  0  0  0  0  0  0 -5  0  0  0  0  0  0  0  0  0  0  0  0  3\n",
      "  0  6  0  0  5  0  0  5  0  0  0 -5  0  0  0  0  0  8  0  0  0  0  0 -1\n",
      "  0  0  5  0  0  0  0  5  0 -5 -8  0  0 -1  0  0  0  3 -5  0 -1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0 -8  0  0\n",
      "  0  5 -1  0  0  0  0  0  6  0  0  0  0  0  0  1  0  0  0  1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  4  0  0  0  0 -5  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0  2  0 -5  0]\n",
      "\n",
      "accuracy=0.806\n"
     ]
    }
   ],
   "source": [
    "# coding= UTF-8\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "#Load data from generated numpy files\n",
    "X = np.load('feat.npy') # list of features\n",
    "y = np.load('label.npy').ravel() # labels are the target\n",
    "\n",
    "# Split into train and test sets (400 Audios total)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# Data scaling (NOT IMPLEMENTING)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\n",
    "X_test_scaled = scaler.transform(X_test.astype(np.float32))\n",
    "\n",
    "# Implement simple linear SVM\n",
    "svm_clf = SVC(C=28.0, gamma = 0.00001, decision_function_shape=\"ovr\") #These parameters can be modified\n",
    "\n",
    "# Fit model\n",
    "svm_clf.fit(X_train, y_train) #From Beif github\n",
    "#svm_clf.fit(X_train_scaled, y_train) # HandsOn book\n",
    "\n",
    "# Make predictions\n",
    "#y_pred = svm_clf.predict(X_train_scaled)\n",
    "y_predict = svm_clf.predict(X_test)\n",
    "\n",
    "#print('Prediction')\n",
    "#print(y_predict)\n",
    "#print\n",
    "#print(\"Actual\")\n",
    "#print(y_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = svm_clf.score(X_test, y_test)\n",
    "print\n",
    "print(\"accuracy=%0.3f\" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy results (Reporting Beif value)\n",
    "\n",
    "- Without data scaling: 0.817 (C = 28)\n",
    "- With data scaling: 0.375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Classification (Keras Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 2.3719 - acc: 0.0958\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 2.2430 - acc: 0.1708\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 2.0834 - acc: 0.2375\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 2.0294 - acc: 0.3375\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.9173 - acc: 0.3000\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.7578 - acc: 0.3792\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.7275 - acc: 0.3875\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.6114 - acc: 0.3792\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.6410 - acc: 0.4083\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 1.5011 - acc: 0.4292\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 1.5212 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.4670 - acc: 0.4958\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 2s 10ms/step - loss: 1.3954 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 1.4814 - acc: 0.4750\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.3728 - acc: 0.4625\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 1.3501 - acc: 0.5250\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.3221 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.2626 - acc: 0.5542\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.3766 - acc: 0.5083\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.2467 - acc: 0.5500\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.1461 - acc: 0.6083\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.1953 - acc: 0.5583\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.2081 - acc: 0.5833\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.1224 - acc: 0.6167\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.1707 - acc: 0.5625\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 1.0616 - acc: 0.6208\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.1028 - acc: 0.6125\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 2s 10ms/step - loss: 1.0843 - acc: 0.6167\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.1226 - acc: 0.5833\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 1.0185 - acc: 0.6250\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 1.1443 - acc: 0.6042\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.0799 - acc: 0.6167\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.9423 - acc: 0.6583\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.9565 - acc: 0.6875\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.9934 - acc: 0.6708\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.9578 - acc: 0.6750\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.8818 - acc: 0.6792\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.0379 - acc: 0.6333\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.8657 - acc: 0.6917\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.8013 - acc: 0.6875\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.8110 - acc: 0.7167\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.8676 - acc: 0.6917\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.8596 - acc: 0.6875\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.8044 - acc: 0.7458\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.8322 - acc: 0.7333\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.9615 - acc: 0.6917\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.8060 - acc: 0.6917\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.7490 - acc: 0.7375\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.8382 - acc: 0.7042\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 2s 6ms/step - loss: 0.7169 - acc: 0.7667\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.7044 - acc: 0.7750\n",
      "Epoch 52/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.6944 - acc: 0.7667\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.7528 - acc: 0.7542\n",
      "Epoch 54/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.6759 - acc: 0.7625\n",
      "Epoch 55/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.6428 - acc: 0.8167\n",
      "Epoch 56/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.7024 - acc: 0.7542\n",
      "Epoch 57/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.6089 - acc: 0.8083\n",
      "Epoch 58/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.6707 - acc: 0.7792\n",
      "Epoch 59/100\n",
      "240/240 [==============================] - 2s 10ms/step - loss: 0.6545 - acc: 0.7792\n",
      "Epoch 60/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.6763 - acc: 0.7458\n",
      "Epoch 61/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.5696 - acc: 0.8083\n",
      "Epoch 62/100\n",
      "240/240 [==============================] - 2s 6ms/step - loss: 0.5558 - acc: 0.8083\n",
      "Epoch 63/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.5857 - acc: 0.8167\n",
      "Epoch 64/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.6678 - acc: 0.7542\n",
      "Epoch 65/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.5745 - acc: 0.8167\n",
      "Epoch 66/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.6491 - acc: 0.7792\n",
      "Epoch 67/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.5932 - acc: 0.8208\n",
      "Epoch 68/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.6431 - acc: 0.7708\n",
      "Epoch 69/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.5504 - acc: 0.8292\n",
      "Epoch 70/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.5317 - acc: 0.8167\n",
      "Epoch 71/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.5693 - acc: 0.8417\n",
      "Epoch 72/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.5538 - acc: 0.8125\n",
      "Epoch 73/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4513 - acc: 0.8583\n",
      "Epoch 74/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.5846 - acc: 0.8208\n",
      "Epoch 75/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.6221 - acc: 0.7792\n",
      "Epoch 76/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.5142 - acc: 0.8500\n",
      "Epoch 77/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4581 - acc: 0.8333\n",
      "Epoch 78/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.4630 - acc: 0.8167\n",
      "Epoch 79/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.5637 - acc: 0.8042\n",
      "Epoch 80/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.4736 - acc: 0.8417\n",
      "Epoch 81/100\n",
      "240/240 [==============================] - 3s 10ms/step - loss: 0.5493 - acc: 0.8542\n",
      "Epoch 82/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.4862 - acc: 0.8125\n",
      "Epoch 83/100\n",
      "240/240 [==============================] - 2s 9ms/step - loss: 0.5151 - acc: 0.8000\n",
      "Epoch 84/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.4493 - acc: 0.8458\n",
      "Epoch 85/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4211 - acc: 0.8625\n",
      "Epoch 86/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4370 - acc: 0.8542\n",
      "Epoch 87/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.3561 - acc: 0.8625\n",
      "Epoch 88/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4851 - acc: 0.8250\n",
      "Epoch 89/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.3951 - acc: 0.8583\n",
      "Epoch 90/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.4081 - acc: 0.8833\n",
      "Epoch 91/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.4388 - acc: 0.8458\n",
      "Epoch 92/100\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.4114 - acc: 0.8667\n",
      "Epoch 93/100\n",
      "240/240 [==============================] - 2s 6ms/step - loss: 0.5450 - acc: 0.8167\n",
      "Epoch 94/100\n",
      "240/240 [==============================] - 2s 6ms/step - loss: 0.3978 - acc: 0.8667\n",
      "Epoch 95/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4619 - acc: 0.8292\n",
      "Epoch 96/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.4081 - acc: 0.8500\n",
      "Epoch 97/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.4346 - acc: 0.8375\n",
      "Epoch 98/100\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.3276 - acc: 0.9083\n",
      "Epoch 99/100\n",
      "240/240 [==============================] - 2s 10ms/step - loss: 0.4764 - acc: 0.8375\n",
      "Epoch 100/100\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.3515 - acc: 0.8750\n",
      "160/160 [==============================] - 0s 3ms/step\n",
      "('Test score:', 0.9385252609848976)\n",
      "('Test accuracy:', 0.7125)\n"
     ]
    }
   ],
   "source": [
    "# coding= UTF-8\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data \n",
    "X = np.load(\"feat.npy\")\n",
    "y = np.load('label.npy').ravel()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 233)\n",
    "\n",
    "# Neural Network Construction\n",
    "model = Sequential()\n",
    "\n",
    "# Architecture\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape = (193, 1)))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Which is the best loss function for binary (multiple) classification\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Convert label to onehot\n",
    "y_train = keras.utils.to_categorical(y_train - 1, num_classes=10) # Converts a class vector (integers) to binary class matrix\n",
    "y_test = keras.utils.to_categorical(y_test - 1, num_classes=10)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2) # Make 2-dim into 3-dim array to fit model\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "# Train Network\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=100)\n",
    "\n",
    "# Compute accuracy with test data\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=16) # Computes the loss & accuracy based on the input you pass it\n",
    "\n",
    "print('Test score:', score) #loss\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 193, 1)\n",
      "(160, 193, 1)\n",
      "(240, 10)\n",
      "(160, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Common Keras NN workflow: Sequential, Add,Compile, Fit\n",
    "- Accuracy with CNN: 0.78125 (1000 epochs)\n",
    "- Accuracy: 0.7125 (100 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron Classification (Keras Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hackerman/anaconda2/envs/anaconda2_py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "240/240 [==============================] - 0s 453us/step - loss: 13.4215 - acc: 0.1125\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 0s 349us/step - loss: 13.3288 - acc: 0.1458\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 0s 350us/step - loss: 11.8501 - acc: 0.2167\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 0s 345us/step - loss: 11.6624 - acc: 0.2125\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 0s 298us/step - loss: 11.1756 - acc: 0.2625\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 0s 200us/step - loss: 10.8737 - acc: 0.2792\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 0s 325us/step - loss: 11.5427 - acc: 0.2500\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 0s 457us/step - loss: 11.3938 - acc: 0.2708\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 0s 441us/step - loss: 11.4216 - acc: 0.2583\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 0s 445us/step - loss: 10.2797 - acc: 0.3167\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 0s 359us/step - loss: 10.4496 - acc: 0.3042\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 0s 302us/step - loss: 10.1510 - acc: 0.3208\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 0s 375us/step - loss: 10.0454 - acc: 0.3333\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 0s 293us/step - loss: 9.7642 - acc: 0.3542\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 0s 503us/step - loss: 9.8325 - acc: 0.3500\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 0s 470us/step - loss: 9.1629 - acc: 0.3917\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 0s 223us/step - loss: 9.2298 - acc: 0.3875\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 0s 320us/step - loss: 9.5320 - acc: 0.3542\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 0s 519us/step - loss: 9.5588 - acc: 0.3625\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 0s 571us/step - loss: 9.2377 - acc: 0.3833\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 0s 982us/step - loss: 9.1258 - acc: 0.3958\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 0s 451us/step - loss: 9.2856 - acc: 0.3875\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 0s 983us/step - loss: 8.9907 - acc: 0.4083\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 0s 516us/step - loss: 8.5812 - acc: 0.4167\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 0s 453us/step - loss: 8.2757 - acc: 0.4500\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 0s 414us/step - loss: 9.0190 - acc: 0.4083\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 0s 527us/step - loss: 8.8084 - acc: 0.4292\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 0s 281us/step - loss: 8.5326 - acc: 0.4417\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 0s 309us/step - loss: 8.1658 - acc: 0.4708\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 0s 621us/step - loss: 8.1866 - acc: 0.4583\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 0s 633us/step - loss: 8.4422 - acc: 0.4500\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 0s 285us/step - loss: 8.0183 - acc: 0.4875\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 0s 289us/step - loss: 8.2826 - acc: 0.4458\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 0s 251us/step - loss: 8.0222 - acc: 0.4542\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 0s 447us/step - loss: 7.9101 - acc: 0.4833\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 0s 394us/step - loss: 7.9020 - acc: 0.4917\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 0s 452us/step - loss: 7.9866 - acc: 0.4583\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 0s 505us/step - loss: 8.1970 - acc: 0.4583\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 0s 512us/step - loss: 7.7172 - acc: 0.5042\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 0s 274us/step - loss: 7.9110 - acc: 0.4708\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 0s 281us/step - loss: 7.8642 - acc: 0.4750\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 0s 219us/step - loss: 7.4677 - acc: 0.5250\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 0s 252us/step - loss: 8.0378 - acc: 0.4625\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 0s 293us/step - loss: 7.6532 - acc: 0.4875\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 0s 323us/step - loss: 7.6289 - acc: 0.5000\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 0s 454us/step - loss: 7.8911 - acc: 0.4958\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 0s 534us/step - loss: 7.8434 - acc: 0.4875\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 0s 254us/step - loss: 7.7016 - acc: 0.4958\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 0s 228us/step - loss: 7.5481 - acc: 0.5167\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 0s 418us/step - loss: 7.5523 - acc: 0.5083\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 0s 810us/step - loss: 7.3688 - acc: 0.5083\n",
      "Epoch 52/100\n",
      "240/240 [==============================] - 0s 740us/step - loss: 7.5473 - acc: 0.5042\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 0s 467us/step - loss: 7.6141 - acc: 0.4875\n",
      "Epoch 54/100\n",
      "240/240 [==============================] - 0s 423us/step - loss: 7.3224 - acc: 0.5333\n",
      "Epoch 55/100\n",
      "240/240 [==============================] - 0s 426us/step - loss: 7.6852 - acc: 0.4833\n",
      "Epoch 56/100\n",
      "240/240 [==============================] - 0s 496us/step - loss: 7.7212 - acc: 0.4958\n",
      "Epoch 57/100\n",
      "240/240 [==============================] - 0s 237us/step - loss: 7.5244 - acc: 0.5125\n",
      "Epoch 58/100\n",
      "240/240 [==============================] - 0s 199us/step - loss: 7.4662 - acc: 0.4917\n",
      "Epoch 59/100\n",
      "240/240 [==============================] - 0s 235us/step - loss: 7.3879 - acc: 0.5250\n",
      "Epoch 60/100\n",
      "240/240 [==============================] - 0s 499us/step - loss: 7.6797 - acc: 0.5042\n",
      "Epoch 61/100\n",
      "240/240 [==============================] - 0s 688us/step - loss: 7.6348 - acc: 0.5208\n",
      "Epoch 62/100\n",
      "240/240 [==============================] - 0s 959us/step - loss: 7.6233 - acc: 0.5125\n",
      "Epoch 63/100\n",
      "240/240 [==============================] - 0s 550us/step - loss: 7.0977 - acc: 0.5292\n",
      "Epoch 64/100\n",
      "240/240 [==============================] - 0s 243us/step - loss: 6.9620 - acc: 0.5333\n",
      "Epoch 65/100\n",
      "240/240 [==============================] - 0s 201us/step - loss: 7.4375 - acc: 0.5042\n",
      "Epoch 66/100\n",
      "240/240 [==============================] - 0s 301us/step - loss: 7.1055 - acc: 0.5417\n",
      "Epoch 67/100\n",
      "240/240 [==============================] - 0s 212us/step - loss: 6.8120 - acc: 0.5500\n",
      "Epoch 68/100\n",
      "240/240 [==============================] - 0s 233us/step - loss: 7.0492 - acc: 0.5125\n",
      "Epoch 69/100\n",
      "240/240 [==============================] - 0s 212us/step - loss: 6.6533 - acc: 0.5667\n",
      "Epoch 70/100\n",
      "240/240 [==============================] - 0s 212us/step - loss: 6.5507 - acc: 0.5667\n",
      "Epoch 71/100\n",
      "240/240 [==============================] - 0s 353us/step - loss: 6.8957 - acc: 0.5292\n",
      "Epoch 72/100\n",
      "240/240 [==============================] - 0s 593us/step - loss: 7.2452 - acc: 0.5125\n",
      "Epoch 73/100\n",
      "240/240 [==============================] - 0s 572us/step - loss: 6.4621 - acc: 0.5750\n",
      "Epoch 74/100\n",
      "240/240 [==============================] - 0s 707us/step - loss: 6.1232 - acc: 0.5708\n",
      "Epoch 75/100\n",
      "240/240 [==============================] - 0s 576us/step - loss: 6.5903 - acc: 0.5583\n",
      "Epoch 76/100\n",
      "240/240 [==============================] - 0s 702us/step - loss: 6.7211 - acc: 0.5458\n",
      "Epoch 77/100\n",
      "240/240 [==============================] - 0s 733us/step - loss: 6.0802 - acc: 0.5875\n",
      "Epoch 78/100\n",
      "240/240 [==============================] - 0s 514us/step - loss: 6.1986 - acc: 0.5833\n",
      "Epoch 79/100\n",
      "240/240 [==============================] - 0s 457us/step - loss: 5.8813 - acc: 0.5958\n",
      "Epoch 80/100\n",
      "240/240 [==============================] - 0s 240us/step - loss: 5.8628 - acc: 0.5917\n",
      "Epoch 81/100\n",
      "240/240 [==============================] - 0s 229us/step - loss: 6.1332 - acc: 0.5667\n",
      "Epoch 82/100\n",
      "240/240 [==============================] - 0s 599us/step - loss: 6.2050 - acc: 0.5708\n",
      "Epoch 83/100\n",
      "240/240 [==============================] - 0s 457us/step - loss: 5.5049 - acc: 0.6208\n",
      "Epoch 84/100\n",
      "240/240 [==============================] - 0s 379us/step - loss: 5.8176 - acc: 0.5875\n",
      "Epoch 85/100\n",
      "240/240 [==============================] - 0s 511us/step - loss: 5.7065 - acc: 0.6167\n",
      "Epoch 86/100\n",
      "240/240 [==============================] - 0s 434us/step - loss: 5.3482 - acc: 0.6250\n",
      "Epoch 87/100\n",
      "240/240 [==============================] - 0s 280us/step - loss: 5.2437 - acc: 0.6333\n",
      "Epoch 88/100\n",
      "240/240 [==============================] - 0s 248us/step - loss: 5.2698 - acc: 0.6167\n",
      "Epoch 89/100\n",
      "240/240 [==============================] - 0s 300us/step - loss: 5.1718 - acc: 0.6375\n",
      "Epoch 90/100\n",
      "240/240 [==============================] - 0s 497us/step - loss: 5.3412 - acc: 0.6292\n",
      "Epoch 91/100\n",
      "240/240 [==============================] - 0s 612us/step - loss: 5.4105 - acc: 0.6250\n",
      "Epoch 92/100\n",
      "240/240 [==============================] - 0s 609us/step - loss: 5.1277 - acc: 0.6417\n",
      "Epoch 93/100\n",
      "240/240 [==============================] - 0s 284us/step - loss: 5.0534 - acc: 0.6333\n",
      "Epoch 94/100\n",
      "240/240 [==============================] - 0s 708us/step - loss: 5.4507 - acc: 0.6375\n",
      "Epoch 95/100\n",
      "240/240 [==============================] - 0s 522us/step - loss: 5.0178 - acc: 0.6542\n",
      "Epoch 96/100\n",
      "240/240 [==============================] - 0s 247us/step - loss: 4.6879 - acc: 0.6875\n",
      "Epoch 97/100\n",
      "240/240 [==============================] - 0s 428us/step - loss: 4.8589 - acc: 0.6792\n",
      "Epoch 98/100\n",
      "240/240 [==============================] - 0s 367us/step - loss: 4.7877 - acc: 0.6750\n",
      "Epoch 99/100\n",
      "240/240 [==============================] - 0s 473us/step - loss: 5.2362 - acc: 0.6292\n",
      "Epoch 100/100\n",
      "240/240 [==============================] - 0s 453us/step - loss: 4.7054 - acc: 0.6667\n",
      "160/160 [==============================] - 0s 165us/step\n",
      "('Test score:', 5.129288291931152)\n",
      "('Test accuracy:', 0.63125)\n"
     ]
    }
   ],
   "source": [
    "# coding= UTF-8\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the data\n",
    "X =  np.load('feat.npy')\n",
    "y =  np.load('label.npy').ravel() #Return a contiguous flattened array.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# Build the Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=193)) ## Dense method for MLP\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Convert label to onehot\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes=10) # Convert class vector into binary Matrix\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes=10)\n",
    "\n",
    "# Train and test\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64) # Epochs are tunable\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy with MLP: 0.80625 (250 epochs)\n",
    "- Accuracy: 0.63125 (100 epochs)\n",
    "- MLP is the faster of the 3 neural networks\n",
    "- A classification predictor can be visualized by drawing the boundary line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (NB) Classification (Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values\n",
      "[6 2 0 9 5 9 8 2 7 3 0 4 1 1 7 1 6 4 8 9 1 0 9 5 8 4 2 5 1 2 9 4 6 2 5 2 2\n",
      " 8 0 3 4 1 6 3 2 0 3 4 0 4 9 6 8 1 4 0 5 4 8 3 3 6 7 7 2 0 8 6 9 1 8 3 0 1\n",
      " 9 6 5 9 4 7 0 3 0 6 1 1 6 7 2 2 5 8 0 5 4 6 9 9 5 1 6 2 7 1 3 0 0 3 8 8 0\n",
      " 4 7 9 7 1 9 7 7 1 0 2 0 8 0 0 8 8 4 6 6 3]\n",
      "\n",
      "Actual values\n",
      "[5 7 0 5 2 2 8 2 6 3 0 9 1 1 3 1 6 9 5 9 1 0 9 6 0 9 2 5 6 2 9 4 6 1 5 2 2\n",
      " 8 3 3 9 1 1 0 2 9 3 4 0 4 1 8 8 1 9 0 5 4 0 3 3 7 6 1 2 0 8 6 9 1 8 3 0 1\n",
      " 9 6 5 9 4 7 0 0 5 6 6 1 6 7 2 2 5 8 2 5 4 7 9 9 5 1 6 2 0 1 3 0 0 3 8 8 3\n",
      " 9 7 9 7 1 1 7 7 7 3 2 0 8 0 4 8 8 4 6 7 3]\n",
      "\n",
      "[ 1 -5  0  4  3  7  0  0  1  0  0 -5  0  0  4  0  0 -5  3  0  0  0  0 -1\n",
      "  8 -5  0  0 -5  0  0  0  0  1  0  0  0  0 -3  0 -5  0  5  3  0 -9  0  0\n",
      "  0  0  8 -2  0  0 -5  0  0  0  8  0  0 -1  1  6  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  3 -5  0 -5  0  0  0  0  0  0  0 -2  0  0 -1\n",
      "  0  0  0  0  0  0  7  0  0  0  0  0  0  0 -3 -5  0  0  0  0  8  0  0 -6\n",
      " -3  0  0  0  0 -4  0  0  0  0 -1  0]\n",
      "\n",
      "Accuracy = 0.697\n"
     ]
    }
   ],
   "source": [
    "# coding= UTF-8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd \n",
    "\n",
    "#Load data \n",
    "X = np.load('feat.npy') \n",
    "y = np.load('label.npy').ravel() \n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Initialize classifier\n",
    "gnb_clf= GaussianNB() #check input params\n",
    "\n",
    "# Train model\n",
    "gnb_clf.fit(X_train, y_train)\n",
    "#model = gnb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "prediction = gnb_clf.predict(X_test)\n",
    "\n",
    "#print('Predicted values')\n",
    "#print(prediction)\n",
    "#print\n",
    "#print('Actual values')\n",
    "#print(y_test)\n",
    "#print\n",
    "\n",
    "# Evaluate accuracy\n",
    "#Similar ways to do it\n",
    "#print(accuracy_score(y_test,prediction)) \n",
    "print\n",
    "acc = gnb_clf.score(X_test, y_test) \n",
    "print(\"Accuracy = %0.3f\" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy with NB: 0.697 (33% Test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification (Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding= UTF-8\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest classifier\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values\n",
      "[5 7 0 9 7 1 8 2 6 3 4 9 1 1 7 1 6 9 3 9 2 0 9 0 0 9 2 5 6 2 9 4 6 1 5 2 2\n",
      " 8 3 3 9 1 6 0 2 9 3 4 3 4 1 6 8 1 9 0 5 4 0 3 3 7 7 2 2 0 8 6 9 1 0 3 0 1\n",
      " 9 6 5 9 4 7 0 4 0 6 1 1 6 7 2 2 5 4 2 5 4 2 4 9 1 1 6 2 7 1 3 0 0 3 3 8 3\n",
      " 9 7 9 7 1 1 7 7 2]\n",
      "\n",
      "Actual values\n",
      "[5 7 0 5 2 2 8 2 6 3 0 9 1 1 3 1 6 9 5 9 1 0 9 6 0 9 2 5 6 2 9 4 6 1 5 2 2\n",
      " 8 3 3 9 1 1 0 2 9 3 4 0 4 1 8 8 1 9 0 5 4 0 3 3 7 6 1 2 0 8 6 9 1 8 3 0 1\n",
      " 9 6 5 9 4 7 0 0 5 6 6 1 6 7 2 2 5 8 2 5 4 7 9 9 5 1 6 2 0 1 3 0 0 3 8 8 3\n",
      " 9 7 9 7 1 1 7 7 7]\n",
      "\n",
      "[ 0  0  0  4  5 -1  0  0  0  0  4  0  0  0  4  0  0  0 -2  0  1  0  0 -6\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0\n",
      "  3  0  0 -2  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0 -8  0\n",
      "  0  0  0  0  0  0  0  0  0  4 -5  0 -5  0  0  0  0  0  0 -4  0  0  0 -5\n",
      " -5  0 -4  0  0  0  7  0  0  0  0  0 -5  0  0  0  0  0  0  0  0  0  0 -5]\n",
      "\n",
      "Accuracy = 0.800\n"
     ]
    }
   ],
   "source": [
    "#Load data \n",
    "X = np.load('feat.npy') \n",
    "y = np.load('label.npy').ravel() \n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize classifier\n",
    "rf_clf = RandomForestClassifier(n_jobs=2, random_state=0) #Check params\n",
    "\n",
    "# Train model\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_prediction = rf_clf.predict(X_test)\n",
    "\n",
    "#print('Predicted values')\n",
    "#print(y_prediction)\n",
    "#print\n",
    "#print('Actual values')\n",
    "#print(y_test)\n",
    "#print\n",
    "\n",
    "# Evaluate accuracy\n",
    "print\n",
    "acc = rf_clf.score(X_test, y_test) \n",
    "print(\"Accuracy = %0.3f\" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy with Random Forest: 0.800 (30% test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0.1, 0. , 0. , 0.8, 0.1, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.2, 0. , 0. , 0. , 0. , 0.8, 0. , 0. ],\n",
       "       [0.7, 0. , 0. , 0.1, 0.1, 0. , 0. , 0. , 0.1, 0. ],\n",
       "       [0. , 0.1, 0. , 0.1, 0.3, 0.1, 0. , 0. , 0. , 0.4],\n",
       "       [0. , 0.1, 0.2, 0. , 0. , 0.2, 0.2, 0.3, 0. , 0. ],\n",
       "       [0. , 0.3, 0.3, 0. , 0. , 0. , 0.3, 0.1, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.1, 0.1, 0. , 0. , 0. , 0.8, 0. ],\n",
       "       [0. , 0.1, 0.8, 0. , 0. , 0.1, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.1, 0.1, 0. , 0. , 0.1, 0.4, 0.2, 0. , 0.1],\n",
       "       [0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the predicted probabilities of the first n observations\n",
    "rf_clf.predict_proba(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction decoded\n",
      "['Person sneeze' 'Chainsaw' 'Dog bark' 'Fire crackling' 'Chainsaw' 'Rain'\n",
      " 'Rooster' 'Sea waves' 'Helicopter' 'Baby cry' 'Clock tick'\n",
      " 'Fire crackling' 'Rain' 'Rain' 'Chainsaw' 'Rain' 'Helicopter'\n",
      " 'Fire crackling' 'Baby cry' 'Fire crackling' 'Sea waves' 'Dog bark'\n",
      " 'Fire crackling' 'Dog bark' 'Dog bark' 'Fire crackling' 'Sea waves'\n",
      " 'Person sneeze' 'Helicopter' 'Sea waves' 'Fire crackling' 'Clock tick'\n",
      " 'Helicopter' 'Rain' 'Person sneeze' 'Sea waves' 'Sea waves' 'Rooster'\n",
      " 'Baby cry' 'Baby cry' 'Fire crackling' 'Rain' 'Helicopter' 'Dog bark'\n",
      " 'Sea waves' 'Fire crackling' 'Baby cry' 'Clock tick' 'Baby cry'\n",
      " 'Clock tick' 'Rain' 'Helicopter' 'Rooster' 'Rain' 'Fire crackling'\n",
      " 'Dog bark' 'Person sneeze' 'Clock tick' 'Dog bark' 'Baby cry' 'Baby cry'\n",
      " 'Chainsaw' 'Chainsaw' 'Sea waves' 'Sea waves' 'Dog bark' 'Rooster'\n",
      " 'Helicopter' 'Fire crackling' 'Rain' 'Dog bark' 'Baby cry' 'Dog bark'\n",
      " 'Rain' 'Fire crackling' 'Helicopter' 'Person sneeze' 'Fire crackling'\n",
      " 'Clock tick' 'Chainsaw' 'Dog bark' 'Clock tick' 'Dog bark' 'Helicopter'\n",
      " 'Rain' 'Rain' 'Helicopter' 'Chainsaw' 'Sea waves' 'Sea waves'\n",
      " 'Person sneeze' 'Clock tick' 'Sea waves' 'Person sneeze' 'Clock tick'\n",
      " 'Sea waves' 'Clock tick' 'Fire crackling' 'Rain' 'Rain' 'Helicopter'\n",
      " 'Sea waves' 'Chainsaw' 'Rain' 'Baby cry' 'Dog bark' 'Dog bark' 'Baby cry'\n",
      " 'Baby cry' 'Rooster' 'Baby cry' 'Fire crackling' 'Chainsaw'\n",
      " 'Fire crackling' 'Chainsaw' 'Rain' 'Rain' 'Chainsaw' 'Chainsaw'\n",
      " 'Sea waves']\n",
      "\n",
      "Actual class decoded\n",
      "['Person sneeze' 'Chainsaw' 'Dog bark' 'Person sneeze' 'Sea waves'\n",
      " 'Sea waves' 'Rooster' 'Sea waves' 'Helicopter' 'Baby cry' 'Dog bark'\n",
      " 'Fire crackling' 'Rain' 'Rain' 'Baby cry' 'Rain' 'Helicopter'\n",
      " 'Fire crackling' 'Person sneeze' 'Fire crackling' 'Rain' 'Dog bark'\n",
      " 'Fire crackling' 'Helicopter' 'Dog bark' 'Fire crackling' 'Sea waves'\n",
      " 'Person sneeze' 'Helicopter' 'Sea waves' 'Fire crackling' 'Clock tick'\n",
      " 'Helicopter' 'Rain' 'Person sneeze' 'Sea waves' 'Sea waves' 'Rooster'\n",
      " 'Baby cry' 'Baby cry' 'Fire crackling' 'Rain' 'Rain' 'Dog bark'\n",
      " 'Sea waves' 'Fire crackling' 'Baby cry' 'Clock tick' 'Dog bark'\n",
      " 'Clock tick' 'Rain' 'Rooster' 'Rooster' 'Rain' 'Fire crackling'\n",
      " 'Dog bark' 'Person sneeze' 'Clock tick' 'Dog bark' 'Baby cry' 'Baby cry'\n",
      " 'Chainsaw' 'Helicopter' 'Rain' 'Sea waves' 'Dog bark' 'Rooster'\n",
      " 'Helicopter' 'Fire crackling' 'Rain' 'Rooster' 'Baby cry' 'Dog bark'\n",
      " 'Rain' 'Fire crackling' 'Helicopter' 'Person sneeze' 'Fire crackling'\n",
      " 'Clock tick' 'Chainsaw' 'Dog bark' 'Dog bark' 'Person sneeze'\n",
      " 'Helicopter' 'Helicopter' 'Rain' 'Helicopter' 'Chainsaw' 'Sea waves'\n",
      " 'Sea waves' 'Person sneeze' 'Rooster' 'Sea waves' 'Person sneeze'\n",
      " 'Clock tick' 'Chainsaw' 'Fire crackling' 'Fire crackling' 'Person sneeze'\n",
      " 'Rain' 'Helicopter' 'Sea waves' 'Dog bark' 'Rain' 'Baby cry' 'Dog bark'\n",
      " 'Dog bark' 'Baby cry' 'Rooster' 'Rooster' 'Baby cry' 'Fire crackling'\n",
      " 'Chainsaw' 'Fire crackling' 'Chainsaw' 'Rain' 'Rain' 'Chainsaw'\n",
      " 'Chainsaw' 'Chainsaw']\n"
     ]
    }
   ],
   "source": [
    "# Dencoding predicted and actual classes (numeric to written)\n",
    "prediction_decoded = label_classes[y_prediction]\n",
    "actual_value_decoded = label_classes[y_test]\n",
    "#print(y_prediction)\n",
    "#print(y_test)\n",
    "print('Prediction decoded')\n",
    "print(prediction_decoded)\n",
    "print\n",
    "print('Actual class decoded')\n",
    "print(actual_value_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Baby cry</th>\n",
       "      <th>Chainsaw</th>\n",
       "      <th>Clock tick</th>\n",
       "      <th>Dog bark</th>\n",
       "      <th>Fire crackling</th>\n",
       "      <th>Helicopter</th>\n",
       "      <th>Person sneeze</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Rooster</th>\n",
       "      <th>Sea waves</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baby cry</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chainsaw</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clock tick</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dog bark</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fire crackling</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helicopter</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Person sneeze</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rain</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rooster</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sea waves</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0           Baby cry  Chainsaw  Clock tick  Dog bark  Fire crackling  \\\n",
       "row_0                                                                      \n",
       "Baby cry              10         1           0         0               0   \n",
       "Chainsaw               0         8           0         0               0   \n",
       "Clock tick             0         0           6         0               0   \n",
       "Dog bark               1         1           2        11               0   \n",
       "Fire crackling         0         0           1         0              15   \n",
       "Helicopter             0         1           0         1               0   \n",
       "Person sneeze          1         0           0         1               1   \n",
       "Rain                   0         0           0         0               0   \n",
       "Rooster                1         0           1         1               0   \n",
       "Sea waves              0         1           0         0               0   \n",
       "\n",
       "col_0           Helicopter  Person sneeze  Rain  Rooster  Sea waves  \n",
       "row_0                                                                \n",
       "Baby cry                 0              0     0        0          0  \n",
       "Chainsaw                 0              0     0        0          2  \n",
       "Clock tick               0              0     0        0          0  \n",
       "Dog bark                 0              0     0        0          0  \n",
       "Fire crackling           0              0     0        0          0  \n",
       "Helicopter               9              0     1        0          0  \n",
       "Person sneeze            0              7     1        0          0  \n",
       "Rain                     1              0    14        0          2  \n",
       "Rooster                  1              0     0        5          0  \n",
       "Sea waves                0              0     1        0         11  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Confusion Matrix\n",
    "pd.crosstab(actual_value_decoded, prediction_decoded)\n",
    "#pd.crosstab(test['species'], preds, rownames=['Actual Species'], colnames=['Predicted Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Classification (Keras Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid vs Softmax: The sigmoid function is used for the two-class logistic regression (0 or 1, speech or non-speech), whereas the softmax function is used for the multiclass logistic regression (a.k.a. MaxEnt, multinomial logistic regression, softmax Regression, Maximum Entropy Classifier). (dog bark, sea waves, ...)\n",
    "- Network Architecture: Regarding more general choices, there is rarely a \"right\" way to construct the architecture. Instead that should be something you test with different meta-params (such as layer sizes, number of layers, amount of drop-out), and should be results-driven (including any limits you might have on resource use for training time/memory use etc).\n",
    "- https://datascience.stackexchange.com/questions/10048/what-is-the-best-keras-model-for-multi-class-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hackerman/anaconda2/envs/anaconda2_py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM RNN model ...\n",
      "Compiling ...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 193, 128)          66560     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 87,498\n",
      "Trainable params: 87,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training ...\n",
      "Epoch 1/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 2.3073 - acc: 0.1300\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 2.2820 - acc: 0.0933\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 2.2082 - acc: 0.2167\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 2.0704 - acc: 0.2900\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 1.9225 - acc: 0.3500\n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 1.8038 - acc: 0.3700\n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 1.7318 - acc: 0.3800\n",
      "Epoch 8/100\n",
      "300/300 [==============================] - 9s 29ms/step - loss: 1.7162 - acc: 0.3867\n",
      "Epoch 9/100\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 1.6172 - acc: 0.4267\n",
      "Epoch 10/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 1.4954 - acc: 0.4367\n",
      "Epoch 11/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 1.4281 - acc: 0.4800\n",
      "Epoch 12/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 1.4210 - acc: 0.4667\n",
      "Epoch 13/100\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 1.3931 - acc: 0.4600\n",
      "Epoch 14/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 1.5051 - acc: 0.4333\n",
      "Epoch 15/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 1.3892 - acc: 0.4900\n",
      "Epoch 16/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 1.3984 - acc: 0.4733\n",
      "Epoch 17/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 1.4233 - acc: 0.4867\n",
      "Epoch 18/100\n",
      "300/300 [==============================] - 12s 38ms/step - loss: 1.3002 - acc: 0.5333\n",
      "Epoch 19/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 1.3092 - acc: 0.5100\n",
      "Epoch 20/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 1.2526 - acc: 0.5467\n",
      "Epoch 21/100\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 1.2740 - acc: 0.5400\n",
      "Epoch 22/100\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 1.2226 - acc: 0.5300\n",
      "Epoch 23/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 1.3185 - acc: 0.4967\n",
      "Epoch 24/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 1.1724 - acc: 0.6033\n",
      "Epoch 25/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 1.1617 - acc: 0.5867\n",
      "Epoch 26/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 1.1778 - acc: 0.5600\n",
      "Epoch 27/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 1.1378 - acc: 0.5900\n",
      "Epoch 28/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 1.0954 - acc: 0.5867\n",
      "Epoch 29/100\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 1.1844 - acc: 0.5600\n",
      "Epoch 30/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 1.0971 - acc: 0.6033\n",
      "Epoch 31/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 1.0456 - acc: 0.6500\n",
      "Epoch 32/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 1.0562 - acc: 0.6400\n",
      "Epoch 33/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 1.0443 - acc: 0.6000\n",
      "Epoch 34/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.9703 - acc: 0.6633\n",
      "Epoch 35/100\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.9346 - acc: 0.6900\n",
      "Epoch 36/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 0.9565 - acc: 0.6400\n",
      "Epoch 37/100\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.9191 - acc: 0.6567\n",
      "Epoch 38/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 0.9065 - acc: 0.6867\n",
      "Epoch 39/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.9202 - acc: 0.6500\n",
      "Epoch 40/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.8696 - acc: 0.6833\n",
      "Epoch 41/100\n",
      "300/300 [==============================] - 11s 37ms/step - loss: 0.8594 - acc: 0.7033\n",
      "Epoch 42/100\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.9398 - acc: 0.6633\n",
      "Epoch 43/100\n",
      "300/300 [==============================] - 11s 37ms/step - loss: 0.9711 - acc: 0.6267\n",
      "Epoch 44/100\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 0.8758 - acc: 0.6933\n",
      "Epoch 45/100\n",
      "300/300 [==============================] - 11s 37ms/step - loss: 0.8874 - acc: 0.7000\n",
      "Epoch 46/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 0.8417 - acc: 0.6933\n",
      "Epoch 47/100\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.8426 - acc: 0.6633\n",
      "Epoch 48/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.7628 - acc: 0.7400\n",
      "Epoch 49/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.7225 - acc: 0.7533\n",
      "Epoch 50/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.7178 - acc: 0.7567\n",
      "Epoch 51/100\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.7088 - acc: 0.7600\n",
      "Epoch 52/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.7406 - acc: 0.7167\n",
      "Epoch 53/100\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.7500 - acc: 0.7367\n",
      "Epoch 54/100\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 0.8444 - acc: 0.6833\n",
      "Epoch 55/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.7868 - acc: 0.7067\n",
      "Epoch 56/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.6757 - acc: 0.7700\n",
      "Epoch 57/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.6754 - acc: 0.7800\n",
      "Epoch 58/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6728 - acc: 0.7733\n",
      "Epoch 59/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6212 - acc: 0.7833\n",
      "Epoch 60/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.5967 - acc: 0.8000\n",
      "Epoch 61/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.6315 - acc: 0.7833\n",
      "Epoch 62/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6031 - acc: 0.7733\n",
      "Epoch 63/100\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.6636 - acc: 0.7433\n",
      "Epoch 64/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6858 - acc: 0.7567\n",
      "Epoch 65/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.7490 - acc: 0.7200\n",
      "Epoch 66/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.7215 - acc: 0.7100\n",
      "Epoch 67/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6442 - acc: 0.7600\n",
      "Epoch 68/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.6616 - acc: 0.7600\n",
      "Epoch 69/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.6564 - acc: 0.7833\n",
      "Epoch 70/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.7383 - acc: 0.7300\n",
      "Epoch 71/100\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.6527 - acc: 0.7667\n",
      "Epoch 72/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.5737 - acc: 0.8000\n",
      "Epoch 73/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5717 - acc: 0.8100\n",
      "Epoch 74/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5688 - acc: 0.8033\n",
      "Epoch 75/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.5224 - acc: 0.8233\n",
      "Epoch 76/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.5583 - acc: 0.7933\n",
      "Epoch 77/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5264 - acc: 0.8100\n",
      "Epoch 78/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.5599 - acc: 0.7967\n",
      "Epoch 79/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4996 - acc: 0.8167\n",
      "Epoch 80/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4795 - acc: 0.8300\n",
      "Epoch 81/100\n",
      "300/300 [==============================] - 9s 29ms/step - loss: 0.4890 - acc: 0.8333\n",
      "Epoch 82/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4902 - acc: 0.8233\n",
      "Epoch 83/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.4932 - acc: 0.8267\n",
      "Epoch 84/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.5312 - acc: 0.8000\n",
      "Epoch 85/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5289 - acc: 0.8067\n",
      "Epoch 86/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5060 - acc: 0.8167\n",
      "Epoch 87/100\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.5374 - acc: 0.8000\n",
      "Epoch 88/100\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.5648 - acc: 0.7833\n",
      "Epoch 89/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5314 - acc: 0.7967\n",
      "Epoch 90/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.5171 - acc: 0.7967\n",
      "Epoch 91/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.4967 - acc: 0.8167\n",
      "Epoch 92/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5171 - acc: 0.8167\n",
      "Epoch 93/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.4616 - acc: 0.8233\n",
      "Epoch 94/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4694 - acc: 0.8367\n",
      "Epoch 95/100\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.4401 - acc: 0.8367\n",
      "Epoch 96/100\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.4349 - acc: 0.8400\n",
      "Epoch 97/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5165 - acc: 0.8233\n",
      "Epoch 98/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5824 - acc: 0.7967\n",
      "Epoch 99/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.6884 - acc: 0.7233\n",
      "Epoch 100/100\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.6126 - acc: 0.7600\n",
      "\n",
      "Validating ...\n",
      "100/100 [==============================] - 1s 9ms/step\n",
      "('Loss:  ', 1.0651737987995147)\n",
      "('Accuracy:  ', 0.6600000202655792)\n"
     ]
    }
   ],
   "source": [
    "# coding= UTF-8\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load data \n",
    "X = np.load(\"feat.npy\")\n",
    "y = np.load('label.npy').ravel()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 233)\n",
    "\n",
    "#batch_size = 35\n",
    "# nb_epochs = 400\n",
    "\n",
    "# Reshape data for LSTM (Samples, Timesteps, Features)\n",
    "X_train = np.expand_dims(X_train, axis=2) #(280,193,1)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train - 1, num_classes=10) # Converts a class vector (integers) to binary class matrix\n",
    "y_test = keras.utils.to_categorical(y_test - 1, num_classes=10)\n",
    "\n",
    "# Build RNN Neural Network\n",
    "print('Build LSTM RNN model ...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "#model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "#model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\n",
    "#model.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\n",
    "#model.add(Dense(units=genre_features.train_Y.shape[1], activation='softmax'))\n",
    "          \n",
    "print(\"Compiling ...\")\n",
    "model.compile(loss='categorical_crossentropy', # for multiple classes\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print(\"Training ...\")\n",
    "model.fit(X_train, y_train, batch_size=35, epochs=100)\n",
    "\n",
    "print(\"\\nValidating ...\")\n",
    "score, accuracy = model.evaluate(X_test, y_test, batch_size=35, verbose=1)\n",
    "print(\"Loss:  \", score)\n",
    "print(\"Accuracy:  \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN Accuracy: 0.660 (25% Test data, 100 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Accuracy Results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN</th>\n",
       "      <th>MLP</th>\n",
       "      <th>NB</th>\n",
       "      <th>RF</th>\n",
       "      <th>RNN</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.125</td>\n",
       "      <td>80.625</td>\n",
       "      <td>69.7</td>\n",
       "      <td>80</td>\n",
       "      <td>66</td>\n",
       "      <td>81.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.250</td>\n",
       "      <td>63.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CNN     MLP    NB  RF  RNN   SVM\n",
       "0  78.125  80.625  69.7  80   66  81.7\n",
       "1  71.250  63.125   0.0   0    0   0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'SVM': [81.7, 0,],\n",
    "                   'CNN': [78.125, 71.25],\n",
    "                   'MLP':[80.625, 63.125],\n",
    "                   'NB': [69.7, 0],\n",
    "                   'RF': [80, 0],\n",
    "                   'RNN': [66, 0],})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 Conda2",
   "language": "python",
   "name": "anaconda2_py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
